{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:10:54.192574Z",
     "start_time": "2024-02-29T10:10:29.380097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7f5d14cf7f241c2b9add36dd52b239e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def perfect_tokenization(text):\n",
    "    # Enhanced split pattern to include \\n and \\t explicitly and treat them as separate tokens\n",
    "    # This pattern also includes ':' now\n",
    "    tokens_with_delimiters = re.split('(\\s|[.,;:?\\n\\t])', text)\n",
    "\n",
    "    # Filter out empty strings from the list to avoid including them as tokens\n",
    "    tokens_with_delimiters = [token for token in tokens_with_delimiters if token]\n",
    "\n",
    "    # Initialize an empty list to store the processed tokens\n",
    "    processed_tokens = []\n",
    "    # Use a flag to mark when the previous character was a space to handle it in the next iteration\n",
    "    was_space = False\n",
    "\n",
    "    for token in tokens_with_delimiters:\n",
    "        # Check if the token is a space or a special whitespace character\n",
    "        if token in [' ', '\\n', '\\t']:\n",
    "            if token == ' ':\n",
    "                was_space = True\n",
    "                continue\n",
    "            else:\n",
    "                # Directly add \\n and \\t as they should be treated as separate tokens\n",
    "                processed_tokens.append(token)\n",
    "        else:\n",
    "            if was_space:\n",
    "                # Prepend 'â–' to indicate a preceding space for non-whitespace tokens\n",
    "                token = 'â–' + token\n",
    "                was_space = False  # Reset the flag after using it\n",
    "\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def actual_tokenization(text, tokenizer):\n",
    "    return tokenizer.tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:59:24.559557Z",
     "start_time": "2024-02-29T10:59:24.554250Z"
    }
   },
   "id": "a6324d6f1cc59f1e"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "text_to_test = \"This is an example. The text demonstrates subword tokenization.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:59:24.876299Z",
     "start_time": "2024-02-29T10:59:24.870036Z"
    }
   },
   "id": "d0b1ed1dc241157e"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'â–is', 'â–an', 'â–example', '.', 'â–The', 'â–text', 'â–demonstrates', 'â–subword', 'â–tokenization', '.']\n",
      "['This', 'â–is', 'â–an', 'â–example', '.', 'â–The', 'â–text', 'â–demonstrates', 'â–sub', 'word', 'â–token', 'ization', '.']\n"
     ]
    }
   ],
   "source": [
    "perfect = perfect_tokenization(text_to_test)\n",
    "print(perfect)\n",
    "actual = actual_tokenization(text_to_test, tokenizer)\n",
    "print(actual)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:59:25.213057Z",
     "start_time": "2024-02-29T10:59:25.207334Z"
    }
   },
   "id": "9da402a122b3403f"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def calculate_metric(perfect_tokens, actual_tokens):\n",
    "    merged_tokens = []  # Stores the result of merging tokens from actual_tokens\n",
    "    i = 0  # Index for iterating over actual_tokens\n",
    "    subwords = 0  # Count of subword merges\n",
    "    words_before_subwording = []  # List to store words that were subworded\n",
    "\n",
    "    while i < len(actual_tokens):\n",
    "        current_token = actual_tokens[i]  # Current token being processed\n",
    "        token_parts = [current_token]  # Parts that will be merged to match a perfect token\n",
    "\n",
    "        # Attempt to merge tokens from actual_tokens to match the current perfect token\n",
    "        while i + 1 < len(actual_tokens) and len(merged_tokens) < len(perfect_tokens) and \\\n",
    "                (current_token != perfect_tokens[len(merged_tokens)] and 'â–' + current_token != perfect_tokens[len(merged_tokens)]):\n",
    "            i += 1\n",
    "            current_token += actual_tokens[i]  # Merge the next token\n",
    "            token_parts.append(actual_tokens[i])  # Keep track of the parts being merged\n",
    "            subwords += 1  # Increment the subword count for each merge\n",
    "\n",
    "        # If more than one part was merged, the original word was subworded\n",
    "        if len(token_parts) > 1:\n",
    "            # Add the original form of the subworded token to the list\n",
    "            # For simplicity, we join the parts with a '+' to indicate they were merged\n",
    "            words_before_subwording.append('+'.join(token_parts))\n",
    "\n",
    "        merged_tokens.append(current_token)\n",
    "        i += 1\n",
    "        \n",
    "    # Calculate the percentage\n",
    "    percentage = (len(perfect_tokens) - subwords) / len(perfect_tokens)\n",
    "\n",
    "    return percentage, words_before_subwording"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:06:19.487689Z",
     "start_time": "2024-02-29T11:06:19.474798Z"
    }
   },
   "id": "aacb1c92b80a3a7c"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8181818181818182"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metric(perfect, actual)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:06:19.780536Z",
     "start_time": "2024-02-29T11:06:19.771966Z"
    }
   },
   "id": "25e331c9eb5729e2"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9550561797752809, ['B+ladder', 'U+reter', 'U+re+thra'])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_test = \"\"\"Bladder: What is the name of the organ that serves as a storage space for urine until it is expelled from the body?\n",
    "Kidney: Which organ is primarily responsible for filtering blood, removing waste, and producing urine in the human body?\n",
    "Ureter: What is the name of the tubes that transport urine from the kidneys to the bladder?\n",
    "Urethra: What is the canal called through which urine is discharged from the bladder and exits the body?\"\"\"\n",
    "p = perfect_tokenization(text_to_test)\n",
    "a = actual_tokenization(text_to_test, tokenizer)\n",
    "calculate_metric(p, a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:08:32.053764Z",
     "start_time": "2024-02-29T11:08:32.029660Z"
    }
   },
   "id": "5c810e79cc3bc9eb"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "complicated_text=\"\"\"The quick, brown fox jumps over the lazy dog; however, unexpected eventsâ€”such as SchrÃ¶dinger's cat paradox in Quantum Mechanics, the Krebs cycle in Biochemistry, and the use of monoclonal antibodies in Immunotherapyâ€”demonstrate the complexity of science. In physics, we explore dark matter and antimatter; in biology, the intricacies of DNA replication and transcription; and in medicine, groundbreaking treatments like CRISPR-Cas9 gene editing. Furthermore, environmental studies on the \"Anthropocene\" epoch highlight human impact. Strange symbols like @, #, $, %, &, *, (, ), [, ], {, }, <, >, +, -, =, |, \\, /, ^, ~, `, and even emoji ğŸ˜Š, challenge tokenization. Quotation marks \"around words,\" apostrophes in contractions (it's, they're), and hyphenated-terms test the tokenizer's limits. Lastly, the incorporation of numbers, such as 3.14 for Ï€, and scientific notations like 6.022e23 for Avogadro's number, along with chemical formulas like H2O, CO2, and C6H12O6, complete this multifaceted evaluation.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:56:40.707398Z",
     "start_time": "2024-02-29T11:56:40.690938Z"
    }
   },
   "id": "c4e7c40e8d75c8f"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.28502415458937197,\n ['â–events+â€”+such',\n  \"â–SchrÃ¶dinger+'+s\",\n  'â–Immun+otherapy+â€”+demon+strate',\n  'â–anti+matter',\n  'â–CRISPR+-+Cas+9',\n  'â–\"+Anthropo+cene+\"',\n  'â–$,+â–%,+â–&,+â–*,+â–(,+â–),+â–[,+â–],+â–{,+â–},+â–<+,+â–>+,+â–+,+â–-,+â–=+,+â–|+,+â–\\\\,+â–/+,+â–^+,+â–~+,+â–`,+â–and+â–even+â–emoji+â–ğŸ˜Š+,+â–challenge+â–token+ization+.+â–Qu+otation+â–marks+â–\"+around+â–words+,\"+â–apost+rophes+â–in+â–contractions+â–(+it+\\'+s+,+â–they+\\'+re+),+â–and+â–hyphen+ated+-+terms+â–test+â–the+â–tokenizer+\\'+s+â–limits+.+â–Lastly+,+â–the+â–incorporation+â–of+â–numbers+,+â–such+â–as+â–+3+.+1+4+â–for+â–Ï€+,+â–and+â–scientific+â–notations+â–like+â–+6+.+0+2+2+e+2+3+â–for+â–Av+og+adro+\\'+s+â–number+,+â–along+â–with+â–chemical+â–formulas+â–like+â–H+2+O+,+â–CO+2+,+â–and+â–C+6+H+1+2+O+6+,+â–complete+â–this+â–multifaceted+â–evaluation+.+\\n'])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = perfect_tokenization(complicated_text)\n",
    "a = actual_tokenization(complicated_text, tokenizer)\n",
    "calculate_metric(p, a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:56:56.295596Z",
     "start_time": "2024-02-29T11:56:56.233661Z"
    }
   },
   "id": "de62fb5865e2b2f7"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8571428571428571, ['â–fast+r'])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_quick = \"fast faster fastest quick quicker quickest\"\n",
    "p = perfect_tokenization(fast_quick)\n",
    "a = actual_tokenization(fast_quick, tokenizer)\n",
    "calculate_metric(p, a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T14:02:32.356550Z",
     "start_time": "2024-03-01T14:02:32.344511Z"
    }
   },
   "id": "ed7888437d96d935"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f387ac364d5c822b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
