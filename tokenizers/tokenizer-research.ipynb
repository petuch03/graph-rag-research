{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:10:54.192574Z",
     "start_time": "2024-02-29T10:10:29.380097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7f5d14cf7f241c2b9add36dd52b239e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def perfect_tokenization(text):\n",
    "    # Enhanced split pattern to include \\n and \\t explicitly and treat them as separate tokens\n",
    "    # This pattern also includes ':' now\n",
    "    tokens_with_delimiters = re.split('(\\s|[.,;:?\\n\\t])', text)\n",
    "\n",
    "    # Filter out empty strings from the list to avoid including them as tokens\n",
    "    tokens_with_delimiters = [token for token in tokens_with_delimiters if token]\n",
    "\n",
    "    # Initialize an empty list to store the processed tokens\n",
    "    processed_tokens = []\n",
    "    # Use a flag to mark when the previous character was a space to handle it in the next iteration\n",
    "    was_space = False\n",
    "\n",
    "    for token in tokens_with_delimiters:\n",
    "        # Check if the token is a space or a special whitespace character\n",
    "        if token in [' ', '\\n', '\\t']:\n",
    "            if token == ' ':\n",
    "                was_space = True\n",
    "                continue\n",
    "            else:\n",
    "                # Directly add \\n and \\t as they should be treated as separate tokens\n",
    "                processed_tokens.append(token)\n",
    "        else:\n",
    "            if was_space:\n",
    "                # Prepend '‚ñÅ' to indicate a preceding space for non-whitespace tokens\n",
    "                token = '‚ñÅ' + token\n",
    "                was_space = False  # Reset the flag after using it\n",
    "\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def actual_tokenization(text, tokenizer):\n",
    "    return tokenizer.tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:59:24.559557Z",
     "start_time": "2024-02-29T10:59:24.554250Z"
    }
   },
   "id": "a6324d6f1cc59f1e"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "text_to_test = \"This is an example. The text demonstrates subword tokenization.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:59:24.876299Z",
     "start_time": "2024-02-29T10:59:24.870036Z"
    }
   },
   "id": "d0b1ed1dc241157e"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', '‚ñÅis', '‚ñÅan', '‚ñÅexample', '.', '‚ñÅThe', '‚ñÅtext', '‚ñÅdemonstrates', '‚ñÅsubword', '‚ñÅtokenization', '.']\n",
      "['This', '‚ñÅis', '‚ñÅan', '‚ñÅexample', '.', '‚ñÅThe', '‚ñÅtext', '‚ñÅdemonstrates', '‚ñÅsub', 'word', '‚ñÅtoken', 'ization', '.']\n"
     ]
    }
   ],
   "source": [
    "perfect = perfect_tokenization(text_to_test)\n",
    "print(perfect)\n",
    "actual = actual_tokenization(text_to_test, tokenizer)\n",
    "print(actual)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T10:59:25.213057Z",
     "start_time": "2024-02-29T10:59:25.207334Z"
    }
   },
   "id": "9da402a122b3403f"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def calculate_metric(perfect_tokens, actual_tokens):\n",
    "    merged_tokens = []  # Stores the result of merging tokens from actual_tokens\n",
    "    i = 0  # Index for iterating over actual_tokens\n",
    "    subwords = 0  # Count of subword merges\n",
    "    words_before_subwording = []  # List to store words that were subworded\n",
    "\n",
    "    while i < len(actual_tokens):\n",
    "        current_token = actual_tokens[i]  # Current token being processed\n",
    "        token_parts = [current_token]  # Parts that will be merged to match a perfect token\n",
    "\n",
    "        # Attempt to merge tokens from actual_tokens to match the current perfect token\n",
    "        while i + 1 < len(actual_tokens) and len(merged_tokens) < len(perfect_tokens) and \\\n",
    "                (current_token != perfect_tokens[len(merged_tokens)] and '‚ñÅ' + current_token != perfect_tokens[len(merged_tokens)]):\n",
    "            i += 1\n",
    "            current_token += actual_tokens[i]  # Merge the next token\n",
    "            token_parts.append(actual_tokens[i])  # Keep track of the parts being merged\n",
    "            subwords += 1  # Increment the subword count for each merge\n",
    "\n",
    "        # If more than one part was merged, the original word was subworded\n",
    "        if len(token_parts) > 1:\n",
    "            # Add the original form of the subworded token to the list\n",
    "            # For simplicity, we join the parts with a '+' to indicate they were merged\n",
    "            words_before_subwording.append('+'.join(token_parts))\n",
    "\n",
    "        merged_tokens.append(current_token)\n",
    "        i += 1\n",
    "        \n",
    "    # Calculate the percentage\n",
    "    percentage = (len(perfect_tokens) - subwords) / len(perfect_tokens)\n",
    "\n",
    "    return percentage, words_before_subwording"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:06:19.487689Z",
     "start_time": "2024-02-29T11:06:19.474798Z"
    }
   },
   "id": "aacb1c92b80a3a7c"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8181818181818182"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metric(perfect, actual)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:06:19.780536Z",
     "start_time": "2024-02-29T11:06:19.771966Z"
    }
   },
   "id": "25e331c9eb5729e2"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9550561797752809, ['B+ladder', 'U+reter', 'U+re+thra'])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_test = \"\"\"Bladder: What is the name of the organ that serves as a storage space for urine until it is expelled from the body?\n",
    "Kidney: Which organ is primarily responsible for filtering blood, removing waste, and producing urine in the human body?\n",
    "Ureter: What is the name of the tubes that transport urine from the kidneys to the bladder?\n",
    "Urethra: What is the canal called through which urine is discharged from the bladder and exits the body?\"\"\"\n",
    "p = perfect_tokenization(text_to_test)\n",
    "a = actual_tokenization(text_to_test, tokenizer)\n",
    "calculate_metric(p, a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:08:32.053764Z",
     "start_time": "2024-02-29T11:08:32.029660Z"
    }
   },
   "id": "5c810e79cc3bc9eb"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "complicated_text=\"\"\"The quick, brown fox jumps over the lazy dog; however, unexpected events‚Äîsuch as Schr√∂dinger's cat paradox in Quantum Mechanics, the Krebs cycle in Biochemistry, and the use of monoclonal antibodies in Immunotherapy‚Äîdemonstrate the complexity of science. In physics, we explore dark matter and antimatter; in biology, the intricacies of DNA replication and transcription; and in medicine, groundbreaking treatments like CRISPR-Cas9 gene editing. Furthermore, environmental studies on the \"Anthropocene\" epoch highlight human impact. Strange symbols like @, #, $, %, &, *, (, ), [, ], {, }, <, >, +, -, =, |, \\, /, ^, ~, `, and even emoji üòä, challenge tokenization. Quotation marks \"around words,\" apostrophes in contractions (it's, they're), and hyphenated-terms test the tokenizer's limits. Lastly, the incorporation of numbers, such as 3.14 for œÄ, and scientific notations like 6.022e23 for Avogadro's number, along with chemical formulas like H2O, CO2, and C6H12O6, complete this multifaceted evaluation.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:56:40.707398Z",
     "start_time": "2024-02-29T11:56:40.690938Z"
    }
   },
   "id": "c4e7c40e8d75c8f"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.28502415458937197,\n ['‚ñÅevents+‚Äî+such',\n  \"‚ñÅSchr√∂dinger+'+s\",\n  '‚ñÅImmun+otherapy+‚Äî+demon+strate',\n  '‚ñÅanti+matter',\n  '‚ñÅCRISPR+-+Cas+9',\n  '‚ñÅ\"+Anthropo+cene+\"',\n  '‚ñÅ$,+‚ñÅ%,+‚ñÅ&,+‚ñÅ*,+‚ñÅ(,+‚ñÅ),+‚ñÅ[,+‚ñÅ],+‚ñÅ{,+‚ñÅ},+‚ñÅ<+,+‚ñÅ>+,+‚ñÅ+,+‚ñÅ-,+‚ñÅ=+,+‚ñÅ|+,+‚ñÅ\\\\,+‚ñÅ/+,+‚ñÅ^+,+‚ñÅ~+,+‚ñÅ`,+‚ñÅand+‚ñÅeven+‚ñÅemoji+‚ñÅüòä+,+‚ñÅchallenge+‚ñÅtoken+ization+.+‚ñÅQu+otation+‚ñÅmarks+‚ñÅ\"+around+‚ñÅwords+,\"+‚ñÅapost+rophes+‚ñÅin+‚ñÅcontractions+‚ñÅ(+it+\\'+s+,+‚ñÅthey+\\'+re+),+‚ñÅand+‚ñÅhyphen+ated+-+terms+‚ñÅtest+‚ñÅthe+‚ñÅtokenizer+\\'+s+‚ñÅlimits+.+‚ñÅLastly+,+‚ñÅthe+‚ñÅincorporation+‚ñÅof+‚ñÅnumbers+,+‚ñÅsuch+‚ñÅas+‚ñÅ+3+.+1+4+‚ñÅfor+‚ñÅœÄ+,+‚ñÅand+‚ñÅscientific+‚ñÅnotations+‚ñÅlike+‚ñÅ+6+.+0+2+2+e+2+3+‚ñÅfor+‚ñÅAv+og+adro+\\'+s+‚ñÅnumber+,+‚ñÅalong+‚ñÅwith+‚ñÅchemical+‚ñÅformulas+‚ñÅlike+‚ñÅH+2+O+,+‚ñÅCO+2+,+‚ñÅand+‚ñÅC+6+H+1+2+O+6+,+‚ñÅcomplete+‚ñÅthis+‚ñÅmultifaceted+‚ñÅevaluation+.+\\n'])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = perfect_tokenization(complicated_text)\n",
    "a = actual_tokenization(complicated_text, tokenizer)\n",
    "calculate_metric(p, a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:56:56.295596Z",
     "start_time": "2024-02-29T11:56:56.233661Z"
    }
   },
   "id": "de62fb5865e2b2f7"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8571428571428571, ['‚ñÅfast+r'])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_quick = \"fast faster fastest quick quicker quickest\"\n",
    "p = perfect_tokenization(fast_quick)\n",
    "a = actual_tokenization(fast_quick, tokenizer)\n",
    "calculate_metric(p, a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T14:02:32.356550Z",
     "start_time": "2024-03-01T14:02:32.344511Z"
    }
   },
   "id": "ed7888437d96d935"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f387ac364d5c822b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
