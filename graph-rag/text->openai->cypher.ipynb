{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "Using ChatOpenAI for extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /Users/es/Library/Python/3.9/lib/python/site-packages (1.12.0)\n",
      "Requirement already satisfied: langchain in /Users/es/Library/Python/3.9/lib/python/site-packages (0.1.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Python/3.9/site-packages (from openai) (4.1.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from openai) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /Library/Python/3.9/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Python/3.9/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Python/3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Python/3.9/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.26)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.19)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (0.1.22)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Python/3.9/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/es/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Python/3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/es/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/es/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Python/3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Library/Python/3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in /Library/Python/3.9/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/es/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/es/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Python/3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/es/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Python/3.9/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Python/3.9/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.0.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (0.1.22)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (1.24.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (1.12.0)\n",
      "Collecting tiktoken<0.6.0,>=0.5.2 (from langchain-openai)\n",
      "  Using cached tiktoken-0.5.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Python/3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Library/Python/3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (4.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.0.88,>=0.0.87 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (0.0.87)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Python/3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /Library/Python/3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Python/3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Python/3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.6.0,>=0.5.2->langchain-openai)\n",
      "  Using cached regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Python/3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Library/Python/3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Library/Python/3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/es/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/es/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Python/3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/es/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/es/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Python/3.9/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Python/3.9/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (2.1.0)\n",
      "Using cached langchain_openai-0.0.5-py3-none-any.whl (29 kB)\n",
      "Using cached tiktoken-0.5.2-cp39-cp39-macosx_11_0_arm64.whl (955 kB)\n",
      "Using cached regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "Installing collected packages: regex, tiktoken, langchain-openai\n",
      "Successfully installed langchain-openai-0.0.5 regex-2023.12.25 tiktoken-0.5.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai langchain\n",
    "!pip3 install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "\n",
    "open_ai_api_key = open('../.open-ai-api-key').read().strip()\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_cypher(text: str) -> str:\n",
    "    chat = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "    example_text = \"\"\"In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs\n",
    "with KGs to conduct faithful and interpretable reasoning. To address the issues of hallucinations and\n",
    "lack of knowledge, we present a planning-retrieval-reasoning framework, where RoG first generates\n",
    "relation paths grounded by KGs as faithful plans via the planning module. These plans are then used\n",
    "to retrieve valid reasoning paths from KGs to conduct faithful reasoning by the retrieval-reasoning\n",
    "module. In this way, we not only retrieve the latest knowledge from KGs but also consider the\n",
    "guidance of KG structure for reasoning and explanations. Moreover, the planning module of RoG\n",
    "can be plug-and-play with different LLMs during inference to improve their performance. Based on\n",
    "this framework, RoG is optimized by two tasks: 1) planning optimization, where we distill knowledge from KGs into LLMs to generate faithful relation paths as plans; and 2) retrieval-reasoning\n",
    "optimization, where we enable LLMs to conduct faithful reasoning based on retrieved paths and generate interpretable results. We conduct extensive experiments on two benchmark KGQA datasets,\n",
    "and the results demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks\n",
    "and generates faithful and interpretable reasoning results.\n",
    "\"\"\"\n",
    "\n",
    "    system_msg = SystemMessage(content=\"You answer must include only code in Cypher without any comments or duplicate expressions.\")\n",
    "    first_shot_msg = HumanMessage(content=f'Your task is to extract objects and relationships' +\n",
    "                     f'from the text in order to create a knowledge graph. Answer only with Cypher query that will put extracted' +\n",
    "                     f'information into knowledge graph.\\nThe example text:\\n{example_text}')\n",
    "    \n",
    "    ai_example_msg = AIMessage(content=\"\"\"CREATE (method:Method {name: \"reasoning on graphs\", description: \"a novel method that synergizes LLMs with KGs to conduct faithful and interpretable reasoning\"})\n",
    "CREATE (framework:Framework {name: \"planning-retrieval-reasoning\", description: \"a framework that generates relation paths grounded by KGs as faithful plans and retrieves valid reasoning paths from KGs to conduct faithful reasoning\"})\n",
    "CREATE (modulePlanning:Module {name: \"planning\", description: \"a module that generates relation paths as plans\"})\n",
    "CREATE (modulePlanning)-[:PART_OF]->(framework)\n",
    "CREATE (framework)-[:USES]->(method)\n",
    "CREATE (moduleRetrievalReasoning:Module {name: \"retrieval-reasoning\", description: \"a module that retrieves reasoning paths from KGs to conduct faithful reasoning\"})\n",
    "CREATE (moduleRetrievalReasoning)-[:PART_OF]->(framework)\n",
    "CREATE (framework)-[:USES]->(method)\n",
    "CREATE (moduleInference:Module {name: \"inference\", description: \"a module that can be plug-and-play with different LLMs to improve their performance\"})\n",
    "CREATE (moduleInference)-[:PART_OF]->(method)\n",
    "CREATE (benchmark:Dataset {name: \"KGQA datasets\", description: \"benchmark datasets for KGQA\"})\n",
    "CREATE (taskPlanningOptimization:Task {name: \"planning optimization\", description: \"optimization task where knowledge is distilled from KGs into LLMs to generate faithful relation paths as plans\"})\n",
    "CREATE (taskPlanningOptimization)-[:OPTIMIZES]->(method)\n",
    "CREATE (taskRetrievalReasoningOptimization:Task {name: \"retrieval-reasoning optimization\", description: \"optimization task where LLMs conduct faithful reasoning based on retrieved paths and generate interpretable results\"})\n",
    "CREATE (taskRetrievalReasoningOptimization)-[:OPTIMIZES]->(method)\n",
    "CREATE (performanceSOTA:PerformanceMetric {name: \"state-of-the-art performance\", description: \"achieving the best performance in KG reasoning tasks\"})\n",
    "CREATE (performanceSOTA)-[:ACHIEVED_BY]->(method)\n",
    "CREATE (performanceFaithful:PerformanceMetric {name: \"faithful and interpretable reasoning results\", description: \"generating reasoning results that are faithful and interpretable\"})\n",
    "CREATE (performanceFaithful)-[:ACHIEVED_BY]->(method)\n",
    "\"\"\")\n",
    "    main_msg = HumanMessage(content=f'Extract objects and relationships' +\n",
    "                     f'from the text in order to create a knowledge graph. Answer only with Cypher query that will put extracted' +\n",
    "                     f'information into knowledge graph.\\nThe text to extract from:\\n{text}')\n",
    "    \n",
    "    answer = chat.invoke([\n",
    "        system_msg,\n",
    "        first_shot_msg,\n",
    "        ai_example_msg,\n",
    "        main_msg\n",
    "    ]).content\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE (runner:Runner {name: \"Kelvin Kiptum Cheruiyot\", nationality: \"Kenyan\", birth_date: date(\"1999-12-02\"), death_date: date(\"2024-02-11\")})\n",
      "CREATE (record:Record {event: \"marathon\", time: \"less than 2:02\", record_type: \"course record\"})\n",
      "CREATE (runner)-[:HOLDS_RECORD]->(record)\n",
      "CREATE (achievement:Achievement {description: \"the only person in history to run the marathon in less than two hours and one minute in a record-eligible race\"})\n",
      "CREATE (runner)-[:ACHIEVED]->(achievement)\n",
      "CREATE (race1:Race {name: \"Valencia Marathon 2022\", date: date(\"2022-10-23\"), time: \"2:02\", rank: \"third\"})\n",
      "CREATE (runner)-[:PARTICIPATED_IN]->(race1)\n",
      "CREATE (race2:Race {name: \"London Marathon 2023\", date: date(\"2023-04-23\"), time: \"2:01:25\", rank: \"second\"})\n",
      "CREATE (runner)-[:PARTICIPATED_IN]->(race2)\n",
      "CREATE (race3:Race {name: \"Chicago Marathon 2023\", date: date(\"2023-10-23\"), time: \"2:00:35\", rank: \"first\"})\n",
      "CREATE (runner)-[:PARTICIPATED_IN]->(race3)\n",
      "CREATE (ranking:Ranking {rank: 1, year: 2024, event: \"men's marathon\"})\n",
      "CREATE (runner)-[:RANKED_FIRST_IN]->(ranking)\n"
     ]
    }
   ],
   "source": [
    "text_for_extraction = \"\"\"\n",
    "Kelvin Kiptum Cheruiyot (2 December 1999 - 11 February 2024) was a Kenyan long-distance runner and the marathon world record holder at the time of his death. He is the only person in history to run the marathon in less than two hours and one minute in a record-eligible race. \n",
    "He ran three of the seven fastest marathons in history.\n",
    "Kiptum won the three marathons he participated in, all renowned, including two top-tier World Marathon Majors (WMM), and held between December 2022 and October 2023. \n",
    "The times he achieved are three of the seven fastest times in history, a course record of less than 2:02 in each case, making him the only person ever to break this barrier thrice.\n",
    "Kiptum ran the fastest-ever marathon debut at the 2022 Valencia Marathon, becoming only the third man in history to break two hours and two minutes and setting the then fourth-quickest time ever. \n",
    "He followed it up four months later with the second-fastest marathon in history at 2:01:25, 16 seconds outside the world record, at the 2023 London Marathon (WMM).\n",
    " In his next race, the 2023 Chicago Marathon (WMM) six months later in October 2023, he broke the world record by 34 seconds with a time of 2:00:35.\n",
    "Kiptum died in a car crash in February 2024. At the time of his death, he was ranked first in the world in the men's marathon rankings.\n",
    "\"\"\"\n",
    "extracted_cypher = text_to_cypher(text_for_extraction)\n",
    "print(extracted_cypher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open questions\n",
    "1. How to avoid creation of duplicate nodes (when we need to create new node and when we need to use existing)?\n",
    "2. How to process big texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 2\n",
    "Using pure llm endpoint for extraction. \n",
    "\n",
    "! to think about: create a tool that will help with conversion into cypher and create an agent for using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "def text_to_cypher_llm(text: str) -> str:\n",
    "    prompt = \"\"\"\n",
    "    Given a detailed description of a research method, framework, and their components, generate Cypher queries to model these as nodes and relationships in a Neo4j graph database. For example:\n",
    "\n",
    "    Description: In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs\n",
    "with KGs to conduct faithful and interpretable reasoning. To address the issues of hallucinations and\n",
    "lack of knowledge, we present a planning-retrieval-reasoning framework, where RoG first generates\n",
    "relation paths grounded by KGs as faithful plans via the planning module. These plans are then used\n",
    "to retrieve valid reasoning paths from KGs to conduct faithful reasoning by the retrieval-reasoning\n",
    "module. In this way, we not only retrieve the latest knowledge from KGs but also consider the\n",
    "guidance of KG structure for reasoning and explanations. Moreover, the planning module of RoG\n",
    "can be plug-and-play with different LLMs during inference to improve their performance. Based on\n",
    "this framework, RoG is optimized by two tasks: 1) planning optimization, where we distill knowledge from KGs into LLMs to generate faithful relation paths as plans; and 2) retrieval-reasoning\n",
    "optimization, where we enable LLMs to conduct faithful reasoning based on retrieved paths and generate interpretable results. We conduct extensive experiments on two benchmark KGQA datasets,\n",
    "and the results demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks\n",
    "and generates faithful and interpretable reasoning results.\n",
    "    Cypher Query: \n",
    "    CREATE (method:Method {name: \"reasoning on graphs\", description: \"a novel method that synergizes LLMs with KGs to conduct faithful and interpretable reasoning\"})\n",
    "    CREATE (framework:Framework {name: \"planning-retrieval-reasoning\", description: \"a framework that generates relation paths grounded by KGs as faithful plans and retrieves valid reasoning paths from KGs to conduct faithful reasoning\"})\n",
    "    [...]\n",
    "\n",
    "    Now, based on the following description, generate the appropriate Cypher query:\n",
    "\n",
    "    \"\"\" + f\"{text}\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE (runner:Runner {name: \"Kelvin Kiptum Cheruiyot\", nationality: \"Kenyan\"})\n",
      "CREATE (record:MarathonRecord {time: \"less than two hours and one minute\", date: \"before February 2024\"})\n",
      "CREATE (runner)-[:HOLDS_RECORD]->(record)\n",
      "CREATE (achievement:MarathonAchievement {description: \"ran three of the seven fastest marathons in history\"})\n",
      "CREATE (runner)-[:ACHIEVES]->(achievement)\n",
      "CREATE (marathon1:Marathon {name: \"Valencia Marathon\", year: 2022})\n",
      "CREATE (marathon2:Marathon {name: \"London Marathon\", year: 2023})\n",
      "CREATE (marathon3:Marathon {name: \"Chicago Marathon\", year: 2023})\n",
      "CREATE (runner)-[:PARTICIPATED_IN]->(marathon1)\n",
      "CREATE (runner)-[:PARTICIPATED_IN]->(marathon2)\n",
      "CREATE (runner)-[:PARTICIPATED_IN]->(marathon3)\n",
      "CREATE (marathon1)-[:FASTEST_TIME]->(time1:Time {time: \"2:02\"})\n",
      "CREATE (marathon2)-[:FASTEST_TIME]->(time2:Time {time: \"2:01:25\"})\n",
      "CREATE (marathon3)-[:FASTEST_TIME]->(time3:Time {time: \"2:00:35\"})\n",
      "CREATE (achievement)-[:ACHIEVES_TIME]->(time1)\n",
      "CREATE (achievement)-[:ACHIEVES_TIME]->(time2)\n",
      "CREATE (achievement)-[:ACHIEVES_TIME]->(time3)\n",
      "CREATE (debut:MarathonDebut {marathon: \"Valencia Marathon\", time: \"2:02\", description: \"fastest-ever marathon debut\"})\n",
      "CREATE (runner)-[:ACHIEVES]->(debut)\n",
      "CREATE (runner)-[:BREAKS_RECORD]->(record)\n",
      "CREATE (crash:CarCrash {date: \"February 2024\"})\n",
      "CREATE (runner)-[:DIES_IN]->(crash)\n",
      "CREATE (ranking:MarathonRanking {rank: 1, year: 2024})\n",
      "CREATE (runner)-[:RANKED_FIRST_IN]->(ranking)\n"
     ]
    }
   ],
   "source": [
    "extracted_cypher_pure_llm = text_to_cypher(text_for_extraction)\n",
    "print(extracted_cypher_pure_llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
